{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare abstracts of papers in my library with new Arxiv submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "## self.text is a term used in linguistics meaning a large set of text body.\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from mylibrary.Keyword_extractor import Keyword_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import mylibrary\n",
    "\n",
    "\n",
    "search_results = requests.get(\"http://export.arxiv.org/api/query?\"+\n",
    "                              \"search_query=cat:astro-ph.GA\"+\n",
    "                              \"&start=0&max_results=10\"+\n",
    "                              \"&sortBy=submittedDate&sortOrder=descending\")\n",
    "\n",
    "alldata=[]\n",
    "\n",
    "root = etree.fromstring(search_results.content)\n",
    "for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "    self = mylibrary.Arxiv.Arxiv_meta()\n",
    "\n",
    "    for element in entry:\n",
    "        name = element.tag.split(\"}\")[-1]\n",
    "        if name == \"author\":\n",
    "            for Echild in element.getchildren():\n",
    "                self.meta[name].append(Echild.text)            \n",
    "        else:\n",
    "            try:\n",
    "                if isinstance(self.meta[name], list):\n",
    "                    try:\n",
    "                        self.meta[name].append(element.attrib[\"term\"])\n",
    "                    except:\n",
    "                        self.meta[name].append(element.text)\n",
    "                else:\n",
    "                    try:\n",
    "                        self.meta[name] = element.attrib[\"term\"]\n",
    "                    except:\n",
    "                        self.meta[name] = element.text\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    alldata.append(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwe = Keyword_extractor()\n",
    "kwe.set_vectorizer(3, max_features=2000)\n",
    "\n",
    "summed_abstract = \"\"\n",
    "for article in alldata:\n",
    "    summed_abstract += article.meta[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwe.extract_words([summed_abstract])\n",
    "top_100_2 = kwe.get_top_n_words(50, ngram=2)\n",
    "top_100_1 = kwe.get_top_n_words(50, ngram=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No abstract\n",
      "No abstract\n"
     ]
    }
   ],
   "source": [
    "import bibtexparser\n",
    "\n",
    "# Extract keywords from my library\n",
    "with open(\"Cluster_env_papers.bib\", \"r\") as f:\n",
    "    bib_db = bibtexparser.load(f)\n",
    "    \n",
    "summed_my_abstract = \"\"\n",
    "for article in bib_db.entries:\n",
    "    try:\n",
    "        summed_my_abstract += article[\"abstract\"]\n",
    "    except:\n",
    "        print(\"No abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwe.extract_words([summed_my_abstract])\n",
    "my_top100_2 = kwe.get_top_n_words(50, ngram=2)\n",
    "my_top100_1 = kwe.get_top_n_words(50, ngram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('galaxy', 20), ('model', 16), ('star', 13), ('arm', 12), ('spiral', 11), ('result', 11), ('grain', 11), ('gas', 10), ('density', 9), ('heating', 9), ('velocity', 8), ('observed', 7), ('formation', 7), ('bulge', 7), ('also', 7), ('stellar', 7), ('data', 7), ('temperature', 7), ('distance', 7), ('two', 6), ('time', 6), ('high', 6), ('winding', 6), ('observation', 6), ('process', 6), ('dispersion', 6), ('sfr', 6), ('surface', 6), ('specie', 6), ('destruction', 6), ('induced', 6), ('scattering', 6), ('evolution', 6), ('accretion', 5), ('cosmic', 5), ('inflow', 5), ('sigma', 5), ('interstellar', 5), ('whole', 5), ('low', 5), ('effective', 5), ('extinction', 5), ('band', 5), ('binary', 5), ('suggests', 4), ('resolution', 4), ('show', 4), ('map', 4), ('correlation', 4), ('size', 4)]\n",
      "[('galaxy', 143), ('cluster', 86), ('mass', 39), ('sample', 28), ('code', 25), ('stellar', 20), ('group', 20), ('distribution', 19), ('formation', 18), ('halo', 18), ('star', 18), ('gas', 17), ('simulation', 17), ('environment', 17), ('evolution', 15), ('show', 15), ('find', 13), ('different', 13), ('core', 13), ('study', 12), ('data', 12), ('radiative', 12), ('local', 12), ('density', 11), ('high', 11), ('also', 11), ('satellite', 11), ('luminosity', 10), ('profile', 10), ('redshift', 10), ('property', 10), ('large', 10), ('range', 10), ('field', 10), ('velocity', 10), ('effect', 10), ('environmental', 10), ('based', 10), ('region', 10), ('present', 9), ('simulated', 9), ('sph', 9), ('within', 9), ('agn', 9), ('result', 9), ('dynamical', 9), ('parameter', 9), ('merger', 9), ('massive', 9), ('entropy', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(top_100_1)\n",
    "print(my_top100_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, I want to compare each new paper to the whole set of my library.\n",
    "\n",
    "https://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-times-recommendation-engine/  \n",
    "\n",
    "1. **Collaborative filtering** cannot be applied to new entries as no user rating is available. (It's true for any paper, as there is no such thing as user rating. --- maybe citation in case of old papers?)  \n",
    "\n",
    "2. **Content-based filtering** also doesn't work very well. If I use pre-defined keywords by journals, I get too broad list of recommendations. I tried to extract more specific keywords, which means now there is no exact one-to-one correlation between keywords of different articles.  \n",
    "\n",
    "3. Then...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet allocation\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation  \n",
    "\n",
    "LDA, an example of topic model, tries to quantify the topic of an article as a combination of *unobserved / latent \n",
    "/ hidden* smaller topics, assuming the article is made up of a few topics.  \n",
    "It can be used, for example, to automatically assign a news articles into one of (culture, politics, economy, science, ...) categories. \n",
    "\n",
    "The name \"Dirichlet\" comes from the assumption that the distribution of topics has a sparse *Dirichlet* distribution. In a layman's term, this means that an article is a mixture of small number of topics, and each \n",
    "topic has a small set of characteristic/distingushing vocabularies.\n",
    "\n",
    "\n",
    "#### Things to consider further\n",
    "\n",
    "Can a scientific paper be well represented by a number of sub topics? Maybe not because there is only central topic in a paper, and subtopics are further details of the central topic. This heirarchy may not be correctly captured by LDA algorithm. \n",
    "\n",
    "My actual thought process when determining which paper to read and which not need to be analyzed before I can go for a specific algorithm....\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
